{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73c50fb",
   "metadata": {},
   "source": [
    "## Test in Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592cd71e-f7a7-49dc-9e9a-237887786e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers~=0.21.4 (from -r ./requirements.txt (line 1))\n",
      "  Using cached diffusers-0.21.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting torch~=2.1.0 (from -r ./requirements.txt (line 2))\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: scipy~=1.11.3 in /opt/conda/lib/python3.10/site-packages (from -r ./requirements.txt (line 3)) (1.11.4)\n",
      "Collecting omegaconf~=2.3.0 (from -r ./requirements.txt (line 4))\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting accelerate~=0.23.0 (from -r ./requirements.txt (line 5))\n",
      "  Using cached accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers~=4.34.0 (from -r ./requirements.txt (line 6))\n",
      "  Using cached transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r ./requirements.txt (line 7)) (4.66.4)\n",
      "Collecting einops (from -r ./requirements.txt (line 8))\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r ./requirements.txt (line 9)) (3.8.4)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio_imageslider (from -r ./requirements.txt (line 11))\n",
      "  Using cached gradio_imageslider-0.0.20-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (0.23.4)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (6.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.10/site-packages (from omegaconf~=2.3.0->-r ./requirements.txt (line 4)) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf~=2.3.0->-r ./requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate~=0.23.0->-r ./requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate~=0.23.0->-r ./requirements.txt (line 5)) (5.9.8)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers~=4.34.0->-r ./requirements.txt (line 6))\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (2.9.0)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (0.110.3)\n",
      "Collecting ffmpy (from gradio->-r ./requirements.txt (line 10))\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.1.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (6.4.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (3.10.4)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (2.1.4)\n",
      "Collecting pydantic>=2.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting pydub (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting urllib3~=2.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (0.30.1)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ./requirements.txt (line 10)) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ./requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ./requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ./requirements.txt (line 10)) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ./requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r ./requirements.txt (line 10)) (0.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ./requirements.txt (line 10)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ./requirements.txt (line 10)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio->-r ./requirements.txt (line 10)) (0.7.0)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=2.0->gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r ./requirements.txt (line 9)) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers~=4.34.0->-r ./requirements.txt (line 6))\n",
      "  Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio_imageslider (from -r ./requirements.txt (line 11))\n",
      "  Using cached gradio_imageslider-0.0.19-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.38.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: altair<6.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (5.3.0)\n",
      "Collecting gradio-client==1.1.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.38.0-py3-none-any.whl.metadata (15 kB)\n",
      "INFO: pip is still looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==1.0.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.37.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.36.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==1.0.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.36.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.35.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.33.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.17.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.17.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.32.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.32.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.32.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.4 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.31.4-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.3 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.31.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.29.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.28.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.28.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.28.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.28.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.27.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.15.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.15.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.26.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (0.9.4)\n",
      "  Using cached gradio-4.25.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.15.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.15.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.24.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.14.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.14.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.23.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.22.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.13.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.13.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.21.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.12.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.12.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.20.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.11.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.11.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.20.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.19.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.10.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.19.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.10.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.19.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.18.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.17.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.9.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.8.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.8.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.15.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.14.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.8.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.13.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.11.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.3 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.10.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Using cached gradio-4.9.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Using cached gradio-4.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.7.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.13.2 (from diffusers~=0.21.4->-r ./requirements.txt (line 1))\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (4.17.3)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers~=0.21.4->-r ./requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers~=0.21.4->-r ./requirements.txt (line 1)) (1.26.19)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (13.7.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio->-r ./requirements.txt (line 10)) (0.37.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers~=0.21.4->-r ./requirements.txt (line 1)) (3.19.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch~=2.1.0->-r ./requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (23.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (0.20.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (0.1.2)\n",
      "Using cached diffusers-0.21.4-py3-none-any.whl (1.5 MB)\n",
      "Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "Using cached transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached gradio-4.8.0-py3-none-any.whl (16.5 MB)\n",
      "Using cached gradio_client-0.7.1-py3-none-any.whl (302 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached gradio_imageslider-0.0.20-py3-none-any.whl (101 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "Installing collected packages: pydub, websockets, triton, tomlkit, semantic-version, python-multipart, pydantic-core, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ffmpy, einops, aiofiles, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, gradio-client, diffusers, transformers, torch, gradio, gradio_imageslider, accelerate\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.12.5\n",
      "    Uninstalling tomlkit-0.12.5:\n",
      "      Successfully uninstalled tomlkit-0.12.5\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.18.4\n",
      "    Uninstalling pydantic_core-2.18.4:\n",
      "      Successfully uninstalled pydantic_core-2.18.4\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.2.3\n",
      "    Uninstalling omegaconf-2.2.3:\n",
      "      Successfully uninstalled omegaconf-2.2.3\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.16\n",
      "    Uninstalling pydantic-1.10.16:\n",
      "      Successfully uninstalled pydantic-1.10.16\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 0.23.4\n",
      "    Uninstalling huggingface_hub-0.23.4:\n",
      "      Successfully uninstalled huggingface_hub-0.23.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.2\n",
      "    Uninstalling transformers-4.40.2:\n",
      "      Successfully uninstalled transformers-4.40.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0.post200\n",
      "    Uninstalling torch-2.0.0.post200:\n",
      "      Successfully uninstalled torch-2.0.0.post200\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 0.23.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.1.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires transformers[sentencepiece]<4.41.0,>=4.36.0, but you have transformers 4.34.1 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.1.2 which is incompatible.\n",
      "datasets 2.19.2 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "gluonts 0.13.7 requires pydantic~=1.7, but you have pydantic 2.8.2 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.18 requires pydantic==1.*, but you have pydantic 2.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.23.0 aiofiles-23.2.1 diffusers-0.21.4 einops-0.8.0 ffmpy-0.4.0 gradio-4.8.0 gradio-client-0.7.1 gradio_imageslider-0.0.20 huggingface-hub-0.17.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 pydantic-2.7.3 pydantic-core-2.20.1 pydub-0.25.1 python-multipart-0.0.9 semantic-version-2.10.0 tokenizers-0.14.1 tomlkit-0.12.0 torch-2.1.2 transformers-4.34.1 triton-2.1.0 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e117f6ad-899f-4971-b707-3bb64dd1dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "from codes.pipeline_demofusion_sdxl import DemoFusionSDXLPipeline\n",
    "import torch, gc\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "PREVIEW_EXT = [\"png\", \"jpeg\", \"jpg\"]\n",
    "GEOTIFF_EXT = [\"tiff\", \"tif\", \"geotiff\"]\n",
    "\n",
    "def load_and_process_image(pil_image):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((1024, 1024)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(pil_image)\n",
    "    image = image.unsqueeze(0).half()\n",
    "    return image\n",
    "\n",
    "\n",
    "def pad_image(image):\n",
    "    w, h = image.size\n",
    "    if w == h:\n",
    "        return image\n",
    "    elif w > h:\n",
    "        new_image = Image.new(image.mode, (w, w), (0, 0, 0))\n",
    "        pad_w = 0\n",
    "        pad_h = (w - h) // 2\n",
    "        new_image.paste(image, (0, pad_h))\n",
    "        return new_image\n",
    "    else:\n",
    "        new_image = Image.new(image.mode, (h, h), (0, 0, 0))\n",
    "        pad_w = (h - w) // 2\n",
    "        pad_h = 0\n",
    "        new_image.paste(image, (pad_w, 0))\n",
    "        return new_image\n",
    "\n",
    "def generate_images(prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, cosine_scale_1, cosine_scale_2, cosine_scale_3, sigma, view_batch_size, stride, seed, image_path):\n",
    "    input_image = Image.open(image_path)\n",
    "    w, h = input_image.size\n",
    "    padded_image = pad_image(input_image).resize((1024, 1024)).convert(\"RGB\")\n",
    "    image_lr = load_and_process_image(padded_image).to('cuda')\n",
    "    vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "    pipe = DemoFusionSDXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", vae=vae, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    generator = torch.Generator(device='cuda')\n",
    "    generator = generator.manual_seed(int(seed))\n",
    "    images = pipe(prompt, negative_prompt=negative_prompt, generator=generator,\n",
    "                  height=int(height), width=int(width), view_batch_size=int(view_batch_size), stride=int(stride),\n",
    "                  num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
    "                  cosine_scale_1=cosine_scale_1, cosine_scale_2=cosine_scale_2, cosine_scale_3=cosine_scale_3, sigma=sigma,\n",
    "                  multi_decoder=True, show_image=False, lowvram=False, image_lr=image_lr\n",
    "                 )    \n",
    "    images_path = list()\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = './tmp/image_'+str(i)+'.png' \n",
    "        images_path.append(image_path)\n",
    "        height, width = image.size\n",
    "        if w < h:\n",
    "            resize_w = int(w * (height / h))\n",
    "            edge = (width - resize_w) // 2\n",
    "            crop_area = (edge, 0, width-edge, height)\n",
    "            cropped_image = image.crop(crop_area)\n",
    "        elif w > h:\n",
    "            resize_h = int(h * (width / w))\n",
    "            edge = (height - resize_h) // 2\n",
    "            crop_area = (0, edge, width, height-edge)\n",
    "            cropped_image = image.crop(crop_area)\n",
    "        else:\n",
    "            cropped_image = image\n",
    "        cropped_image.save(image_path)\n",
    "    pipe = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return images_path\n",
    "\n",
    "\n",
    "def download_from_s3(file_path, bucket, key, region):\n",
    "    # if file_path exists, no need to download\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"{} exists already\".format(file_path))\n",
    "        return\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "    print(\"Downloading s3://{}/{} to {}...\".format(bucket, key, file_path))\n",
    "    s3.download_file(bucket, key, file_path)\n",
    "    print(\"S3 download successful! \\n\")\n",
    "\n",
    "\n",
    "def upload_to_s3(file_path, bucket, key, region):\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    _extension = file_path.split(\".\")[-1]\n",
    "    if _extension == \"png\":\n",
    "        content_type = \"image/png\"\n",
    "    elif _extension in [\"jpeg\", \"jpg\"]:\n",
    "        content_type = \"image/jpeg\"\n",
    "    else:\n",
    "        content_type = \"image/tiff\"\n",
    "    print(\"Uploading to s3://{}/{}...\".format(bucket, key))\n",
    "    s3.upload_file(file_path, bucket, key, ExtraArgs={\"ContentType\": content_type})\n",
    "    print(\"S3 upload successful! \\n\")\n",
    "\n",
    "\n",
    "def process_input(data):\n",
    "    if not os.path.isdir(\"./tmp\"):\n",
    "        os.mkdir(\"./tmp\")\n",
    "\n",
    "    if isinstance(data, str):\n",
    "        model_input = json.loads(data)\n",
    "    else:\n",
    "        model_input = json.loads(data.read().decode(\"utf-8\"))\n",
    "\n",
    "    print(\"Body:\", model_input)\n",
    "\n",
    "    prompt = model_input[\"prompt\"]\n",
    "    negative_prompt = model_input[\"negative_prompt\"]\n",
    "    width = model_input[\"width\"]\n",
    "    height = model_input[\"height\"]\n",
    "    num_inference_steps = model_input[\"num_inference_steps\"]\n",
    "    guidance_scale = model_input[\"guidance_scale\"]\n",
    "    cosine_scale_1 = model_input[\"cosine_scale_1\"]\n",
    "    cosine_scale_2 = model_input[\"cosine_scale_2\"]\n",
    "    cosine_scale_3 = model_input[\"cosine_scale_3\"]\n",
    "    sigma = model_input[\"sigma\"]\n",
    "    view_batch_size = model_input[\"view_batch_size\"]\n",
    "    stride = model_input[\"stride\"]\n",
    "    seed = model_input[\"seed\"]\n",
    "    bucket = model_input[\"bucket\"]\n",
    "    key = model_input[\"key\"]\n",
    "    region = model_input[\"region\"]\n",
    "\n",
    "\n",
    "    filename = key.split(\"/\")[-1]\n",
    "    local_path =\"./tmp/\"+ filename\n",
    "    download_from_s3(local_path, bucket, key, region)\n",
    "\n",
    "    images_path = generate_images(prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, cosine_scale_1, cosine_scale_2, cosine_scale_3, sigma, view_batch_size, stride, seed, local_path)\n",
    "\n",
    "    return images_path, model_input\n",
    "\n",
    "\n",
    "def process_output(model_input, images_path):\n",
    "    response = {}\n",
    "    response[\"predictions\"] = []\n",
    "    bucket = model_input[\"bucket\"]\n",
    "    region = model_input[\"region\"]\n",
    "    for image_path in images_path:\n",
    "        image_name = image_path.split(\"/\")[-1]\n",
    "        key = 'results/' + image_name\n",
    "        upload_to_s3(image_path, bucket, key, region)\n",
    "        single_response = {\n",
    "            \"image_s3_path\" : {\n",
    "                \"bucket\" : bucket,\n",
    "                \"region\" : region,\n",
    "                \"key\" : key,\n",
    "\n",
    "            },\n",
    "        }\n",
    "        response[\"predictions\"].append(single_response)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def handler(data, context):\n",
    "   \"\"\"\n",
    "   data:\n",
    "   {\n",
    "        \"image_input\":\n",
    "        \"prompt\":\n",
    "        \"negative_prompt\":\n",
    "        \"weight\":\n",
    "        \"height\":\n",
    "        \"num_inference_steps\":\n",
    "        \"guidance_scale\":\n",
    "        \"cosine_scale_1\":\n",
    "        \"cosine_scale_2\":\n",
    "        \"cosine_scale_3\":\n",
    "        \"sigma\":\n",
    "        \"seed\":\n",
    "        \"bucket\":\n",
    "        \"region\":\n",
    "        \"key\":\n",
    "\n",
    "   } \n",
    "   \"\"\"\n",
    "   images_path, model_input = process_input(data)\n",
    "   response = process_output(model_input, images_path)\n",
    "\n",
    "   return json.dumps(response, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2432f1-9f18-4b3f-9f25-9d90f6d5079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 30 20:15:13 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   36C    P8              11W /  72W |      0MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7090e6e3-bdc2-421e-b0dd-cba8983d1230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body: {'prompt': 'a satellite image', 'negative_prompt': 'blurry, ugly, duplicate, poorly drawn, deformed, mosaic', 'width': 2048, 'height': 2048, 'num_inference_steps': 50, 'guidance_scale': 7.5, 'cosine_scale_1': 3, 'cosine_scale_2': 1, 'cosine_scale_3': 1, 'sigma': 0.8, 'view_batch_size': 16, 'stride': 64, 'seed': 2013, 'bucket': 'test-aws-mybucket', 'region': 'us-west-2', 'key': 'data/sample.png'}\n",
      "./tmp/sample.png exists already\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11062d8cf92440e7a96dbe2ad98fbf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Encoding Real Image ###\n",
      "### Phase 1 Denoising ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd09f35f665b490d8b634d64ec66f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Phase 1 Decoding ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6a57728900436d9cf6346f7e91b8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Phase 2 Denoising ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73d26305c62427bbcc9498280ec4cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Phase 2 Decoding ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f7a7f393da4b47aca99784473c24a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to s3://test-aws-mybucket/results/image_0.png...\n",
      "S3 upload successful! \n",
      "\n",
      "Uploading to s3://test-aws-mybucket/results/image_1.png...\n",
      "S3 upload successful! \n",
      "\n",
      "Uploading to s3://test-aws-mybucket/results/image_2.png...\n",
      "S3 upload successful! \n",
      "\n",
      "{\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"image_s3_path\": {\n",
      "        \"bucket\": \"test-aws-mybucket\",\n",
      "        \"region\": \"us-west-2\",\n",
      "        \"key\": \"results/image_0.png\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"image_s3_path\": {\n",
      "        \"bucket\": \"test-aws-mybucket\",\n",
      "        \"region\": \"us-west-2\",\n",
      "        \"key\": \"results/image_1.png\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"image_s3_path\": {\n",
      "        \"bucket\": \"test-aws-mybucket\",\n",
      "        \"region\": \"us-west-2\",\n",
      "        \"key\": \"results/image_2.png\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "payload = { \n",
    "    \"prompt\": \"a satellite image\",\n",
    "    \"negative_prompt\": \"blurry, ugly, duplicate, poorly drawn, deformed, mosaic\",\n",
    "    \"width\": 2048,\n",
    "    \"height\": 2048,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"cosine_scale_1\": 3,\n",
    "    \"cosine_scale_2\": 1,\n",
    "    \"cosine_scale_3\": 1,\n",
    "    \"sigma\": 0.8,\n",
    "    \"view_batch_size\": 16,\n",
    "    \"stride\": 64,\n",
    "    \"seed\": 2013,\n",
    "    \"bucket\": \"test-aws-mybucket\",\n",
    "    \"region\": \"us-west-2\",\n",
    "    \"key\": \"data/sample.png\",\n",
    "}\n",
    "\n",
    "print(handler(json.dumps(payload), None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da17140-7cc4-4e4c-9fad-384af0097e2a",
   "metadata": {},
   "source": [
    "## Inference Endpoint Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fca77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "# Specify your AWS Region\n",
    "aws_region='us-west-2'\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Role to give SageMaker permission to access AWS services.\n",
    "sagemaker_role= \"arn:aws:iam::573944535954:role/Super_Resolution\"\n",
    "\n",
    "ecr_image = \"573944535954.dkr.ecr.us-west-2.amazonaws.com/super-resolution:latest\"\n",
    "instance_type = \"ml.g5.xlarge\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1acc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'demofusion-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': ecr_image,\n",
    "    },\n",
    "    ExecutionRoleArn = sagemaker_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77bbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the endpoint configuration associated with this endpoint.\n",
    "endpoint_config_name = \"superresolution-demofusion-config\"\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\", # The name of the production variant.\n",
    "            \"ModelName\": model_name, \n",
    "            \"InstanceType\": instance_type, # Specify the compute instance type.\n",
    "            \"InitialInstanceCount\": 1, # Number of instances to launch initially.\n",
    "        }\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            # Location to upload response outputs when no location is provided in the request.\n",
    "            \"S3OutputPath\": \"s3://test-aws-mybucket/results/\",\n",
    "            \"S3FailurePath\": \"s3://test-aws-mybucket/results/\"\n",
    "            },        \n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a64b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Status: Creating\n",
      "Waiting for super-resolution-demofusion endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "# The name of the endpoint.The name must be unique within an AWS Region in your AWS account.\n",
    "endpoint_name = 'super-resolution-demofusion' \n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "                                            EndpointName=endpoint_name, \n",
    "                                            EndpointConfigName=endpoint_config_name) \n",
    "\n",
    "resp = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab625966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=aws_region)\n",
    "\n",
    "# Specify the location of the input. Here, a single SVM sample\n",
    "input_location = \"s3://test-aws-mybucket/payload.json\"\n",
    "\n",
    "\n",
    "# After you deploy a model into production using SageMaker hosting \n",
    "# services, your client applications use this API to get inferences \n",
    "# from the model hosted at the specified endpoint.\n",
    "response = sagemaker_runtime.invoke_endpoint_async(\n",
    "                            EndpointName=endpoint_name, \n",
    "                            ContentType='application/json',\n",
    "                            InputLocation=input_location,\n",
    "                            InvocationTimeoutSeconds=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fde6cb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"super-resolution-demofusion\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m endpoint_config_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuperresolution-demofusion-config\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m endpoint_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuper-resolution-demofusion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sagemaker_client\u001b[38;5;241m.\u001b[39mdelete_endpoint_config(EndpointConfigName\u001b[38;5;241m=\u001b[39mendpoint_config_name)\n\u001b[1;32m      6\u001b[0m sagemaker_client\u001b[38;5;241m.\u001b[39mdelete_model(ModelName\u001b[38;5;241m=\u001b[39mmodel_name)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"super-resolution-demofusion\"."
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"superresolution-demofusion-config\"\n",
    "endpoint_name = 'super-resolution-demofusion'\n",
    "\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sagemaker_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
