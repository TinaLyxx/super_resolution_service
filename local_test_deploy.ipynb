{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73c50fb",
   "metadata": {},
   "source": [
    "## Test in Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592cd71e-f7a7-49dc-9e9a-237887786e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers~=0.21.4 (from -r ./requirements.txt (line 1))\n",
      "  Using cached diffusers-0.21.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting torch~=2.1.0 (from -r ./requirements.txt (line 2))\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: scipy~=1.11.3 in /opt/conda/lib/python3.10/site-packages (from -r ./requirements.txt (line 3)) (1.11.4)\n",
      "Collecting omegaconf~=2.3.0 (from -r ./requirements.txt (line 4))\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting accelerate~=0.23.0 (from -r ./requirements.txt (line 5))\n",
      "  Using cached accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers~=4.34.0 (from -r ./requirements.txt (line 6))\n",
      "  Using cached transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r ./requirements.txt (line 7)) (4.66.4)\n",
      "Collecting einops (from -r ./requirements.txt (line 8))\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r ./requirements.txt (line 9)) (3.8.4)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio_imageslider (from -r ./requirements.txt (line 11))\n",
      "  Using cached gradio_imageslider-0.0.20-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (0.23.4)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (6.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers~=0.21.4->-r ./requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch~=2.1.0->-r ./requirements.txt (line 2)) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.0->-r ./requirements.txt (line 2))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.10/site-packages (from omegaconf~=2.3.0->-r ./requirements.txt (line 4)) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf~=2.3.0->-r ./requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate~=0.23.0->-r ./requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate~=0.23.0->-r ./requirements.txt (line 5)) (5.9.8)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers~=4.34.0->-r ./requirements.txt (line 6))\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r ./requirements.txt (line 9)) (2.9.0)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (0.110.3)\n",
      "Collecting ffmpy (from gradio->-r ./requirements.txt (line 10))\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.1.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (6.4.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (3.10.4)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (2.1.4)\n",
      "Collecting pydantic>=2.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting pydub (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting urllib3~=2.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (0.30.1)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ./requirements.txt (line 10)) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ./requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ./requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ./requirements.txt (line 10)) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ./requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r ./requirements.txt (line 10)) (0.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ./requirements.txt (line 10)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ./requirements.txt (line 10)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio->-r ./requirements.txt (line 10)) (0.7.0)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=2.0->gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r ./requirements.txt (line 9)) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers~=4.34.0->-r ./requirements.txt (line 6))\n",
      "  Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio_imageslider (from -r ./requirements.txt (line 11))\n",
      "  Using cached gradio_imageslider-0.0.19-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.38.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: altair<6.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ./requirements.txt (line 10)) (5.3.0)\n",
      "Collecting gradio-client==1.1.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.38.0-py3-none-any.whl.metadata (15 kB)\n",
      "INFO: pip is still looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==1.0.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.37.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.36.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==1.0.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-1.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.36.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.35.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.33.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.17.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.17.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.32.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.32.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.32.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.4 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.31.4-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.3 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.31.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.31.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.29.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.28.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.16.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.16.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.28.2-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.28.1-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.28.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.27.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.15.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.15.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.26.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (0.9.4)\n",
      "  Using cached gradio-4.25.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.15.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.15.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.24.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.14.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.14.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.23.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.22.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.13.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.13.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.21.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.12.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.12.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.20.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.11.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.11.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.20.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.19.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.10.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.19.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.10.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.19.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.18.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.17.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.9.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.8.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.8.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.15.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.14.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==0.8.0 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.13.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "  Using cached gradio-4.11.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.3 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.10.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Using cached gradio-4.9.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Using cached gradio-4.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.2 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.7.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from -r ./requirements.txt (line 10))\n",
      "  Using cached gradio-4.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.1 (from gradio->-r ./requirements.txt (line 10))\n",
      "  Using cached gradio_client-0.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.13.2 (from diffusers~=0.21.4->-r ./requirements.txt (line 1))\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (4.17.3)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers~=0.21.4->-r ./requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers~=0.21.4->-r ./requirements.txt (line 1)) (1.26.19)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (13.7.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio->-r ./requirements.txt (line 10)) (0.37.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers~=0.21.4->-r ./requirements.txt (line 1)) (3.19.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch~=2.1.0->-r ./requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (23.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio->-r ./requirements.txt (line 10)) (0.20.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio->-r ./requirements.txt (line 10)) (0.1.2)\n",
      "Using cached diffusers-0.21.4-py3-none-any.whl (1.5 MB)\n",
      "Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "Using cached transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached gradio-4.8.0-py3-none-any.whl (16.5 MB)\n",
      "Using cached gradio_client-0.7.1-py3-none-any.whl (302 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached gradio_imageslider-0.0.20-py3-none-any.whl (101 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "Installing collected packages: pydub, websockets, triton, tomlkit, semantic-version, python-multipart, pydantic-core, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ffmpy, einops, aiofiles, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, gradio-client, diffusers, transformers, torch, gradio, gradio_imageslider, accelerate\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.12.5\n",
      "    Uninstalling tomlkit-0.12.5:\n",
      "      Successfully uninstalled tomlkit-0.12.5\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.18.4\n",
      "    Uninstalling pydantic_core-2.18.4:\n",
      "      Successfully uninstalled pydantic_core-2.18.4\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.2.3\n",
      "    Uninstalling omegaconf-2.2.3:\n",
      "      Successfully uninstalled omegaconf-2.2.3\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.16\n",
      "    Uninstalling pydantic-1.10.16:\n",
      "      Successfully uninstalled pydantic-1.10.16\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 0.23.4\n",
      "    Uninstalling huggingface_hub-0.23.4:\n",
      "      Successfully uninstalled huggingface_hub-0.23.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.2\n",
      "    Uninstalling transformers-4.40.2:\n",
      "      Successfully uninstalled transformers-4.40.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0.post200\n",
      "    Uninstalling torch-2.0.0.post200:\n",
      "      Successfully uninstalled torch-2.0.0.post200\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.21.0\n",
      "    Uninstalling accelerate-0.21.0:\n",
      "      Successfully uninstalled accelerate-0.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 0.8.3 requires accelerate<0.22.0,>=0.21.0, but you have accelerate 0.23.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.1.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires transformers[sentencepiece]<4.41.0,>=4.36.0, but you have transformers 4.34.1 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.1.2 which is incompatible.\n",
      "datasets 2.19.2 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "gluonts 0.13.7 requires pydantic~=1.7, but you have pydantic 2.8.2 which is incompatible.\n",
      "sagemaker-jupyterlab-extension-common 0.1.18 requires pydantic==1.*, but you have pydantic 2.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.23.0 aiofiles-23.2.1 diffusers-0.21.4 einops-0.8.0 ffmpy-0.4.0 gradio-4.8.0 gradio-client-0.7.1 gradio_imageslider-0.0.20 huggingface-hub-0.17.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 pydantic-2.7.3 pydantic-core-2.20.1 pydub-0.25.1 python-multipart-0.0.9 semantic-version-2.10.0 tokenizers-0.14.1 tomlkit-0.12.0 torch-2.1.2 transformers-4.34.1 triton-2.1.0 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e117f6ad-899f-4971-b707-3bb64dd1dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "from codes.pipeline_demofusion_sdxl import DemoFusionSDXLPipeline\n",
    "import torch, gc\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "PREVIEW_EXT = [\"png\", \"jpeg\", \"jpg\"]\n",
    "GEOTIFF_EXT = [\"tiff\", \"tif\", \"geotiff\"]\n",
    "\n",
    "def load_and_process_image(pil_image):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((1024, 1024)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "    image = transform(pil_image)\n",
    "    image = image.unsqueeze(0).half()\n",
    "    return image\n",
    "\n",
    "\n",
    "def pad_image(image):\n",
    "    w, h = image.size\n",
    "    if w == h:\n",
    "        return image\n",
    "    elif w > h:\n",
    "        new_image = Image.new(image.mode, (w, w), (0, 0, 0))\n",
    "        pad_w = 0\n",
    "        pad_h = (w - h) // 2\n",
    "        new_image.paste(image, (0, pad_h))\n",
    "        return new_image\n",
    "    else:\n",
    "        new_image = Image.new(image.mode, (h, h), (0, 0, 0))\n",
    "        pad_w = (h - w) // 2\n",
    "        pad_h = 0\n",
    "        new_image.paste(image, (pad_w, 0))\n",
    "        return new_image\n",
    "\n",
    "def generate_images(prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, cosine_scale_1, cosine_scale_2, cosine_scale_3, sigma, view_batch_size, stride, seed, image_path):\n",
    "    input_image = Image.open(image_path)\n",
    "    w, h = input_image.size\n",
    "    padded_image = pad_image(input_image).resize((1024, 1024)).convert(\"RGB\")\n",
    "    image_lr = load_and_process_image(padded_image).to('cuda')\n",
    "    vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "    pipe = DemoFusionSDXLPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", vae=vae, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    generator = torch.Generator(device='cuda')\n",
    "    generator = generator.manual_seed(int(seed))\n",
    "    images = pipe(prompt, negative_prompt=negative_prompt, generator=generator,\n",
    "                  height=int(height), width=int(width), view_batch_size=int(view_batch_size), stride=int(stride),\n",
    "                  num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
    "                  cosine_scale_1=cosine_scale_1, cosine_scale_2=cosine_scale_2, cosine_scale_3=cosine_scale_3, sigma=sigma,\n",
    "                  multi_decoder=True, show_image=False, lowvram=False, image_lr=image_lr\n",
    "                 )    \n",
    "    images_path = list()\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = './tmp/image_'+str(i)+'.png' \n",
    "        images_path.append(image_path)\n",
    "        height, width = image.size\n",
    "        if w < h:\n",
    "            resize_w = int(w * (height / h))\n",
    "            edge = (width - resize_w) // 2\n",
    "            crop_area = (edge, 0, width-edge, height)\n",
    "            cropped_image = image.crop(crop_area)\n",
    "        elif w > h:\n",
    "            resize_h = int(h * (width / w))\n",
    "            edge = (height - resize_h) // 2\n",
    "            crop_area = (0, edge, width, height-edge)\n",
    "            cropped_image = image.crop(crop_area)\n",
    "        else:\n",
    "            cropped_image = image\n",
    "        cropped_image.save(image_path)\n",
    "    pipe = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return images_path\n",
    "\n",
    "\n",
    "def download_from_s3(file_path, bucket, key, region):\n",
    "    # if file_path exists, no need to download\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"{} exists already\".format(file_path))\n",
    "        return\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "    print(\"Downloading s3://{}/{} to {}...\".format(bucket, key, file_path))\n",
    "    s3.download_file(bucket, key, file_path)\n",
    "    print(\"S3 download successful! \\n\")\n",
    "\n",
    "\n",
    "def upload_to_s3(file_path, bucket, key, region):\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    _extension = file_path.split(\".\")[-1]\n",
    "    if _extension == \"png\":\n",
    "        content_type = \"image/png\"\n",
    "    elif _extension in [\"jpeg\", \"jpg\"]:\n",
    "        content_type = \"image/jpeg\"\n",
    "    else:\n",
    "        content_type = \"image/tiff\"\n",
    "    print(\"Uploading to s3://{}/{}...\".format(bucket, key))\n",
    "    s3.upload_file(file_path, bucket, key, ExtraArgs={\"ContentType\": content_type})\n",
    "    print(\"S3 upload successful! \\n\")\n",
    "\n",
    "\n",
    "def process_input(data):\n",
    "    if not os.path.isdir(\"./tmp\"):\n",
    "        os.mkdir(\"./tmp\")\n",
    "\n",
    "    if isinstance(data, str):\n",
    "        model_input = json.loads(data)\n",
    "    else:\n",
    "        model_input = json.loads(data.read().decode(\"utf-8\"))\n",
    "\n",
    "    print(\"Body:\", model_input)\n",
    "\n",
    "    prompt = model_input[\"prompt\"]\n",
    "    negative_prompt = model_input[\"negative_prompt\"]\n",
    "    width = model_input[\"width\"]\n",
    "    height = model_input[\"height\"]\n",
    "    num_inference_steps = model_input[\"num_inference_steps\"]\n",
    "    guidance_scale = model_input[\"guidance_scale\"]\n",
    "    cosine_scale_1 = model_input[\"cosine_scale_1\"]\n",
    "    cosine_scale_2 = model_input[\"cosine_scale_2\"]\n",
    "    cosine_scale_3 = model_input[\"cosine_scale_3\"]\n",
    "    sigma = model_input[\"sigma\"]\n",
    "    view_batch_size = model_input[\"view_batch_size\"]\n",
    "    stride = model_input[\"stride\"]\n",
    "    seed = model_input[\"seed\"]\n",
    "    bucket = model_input[\"bucket\"]\n",
    "    key = model_input[\"key\"]\n",
    "    region = model_input[\"region\"]\n",
    "\n",
    "\n",
    "    filename = key.split(\"/\")[-1]\n",
    "    local_path =\"./tmp/\"+ filename\n",
    "    download_from_s3(local_path, bucket, key, region)\n",
    "\n",
    "    images_path = generate_images(prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, cosine_scale_1, cosine_scale_2, cosine_scale_3, sigma, view_batch_size, stride, seed, local_path)\n",
    "\n",
    "    return images_path, model_input\n",
    "\n",
    "\n",
    "def process_output(model_input, images_path):\n",
    "    response = {}\n",
    "    response[\"predictions\"] = []\n",
    "    bucket = model_input[\"bucket\"]\n",
    "    region = model_input[\"region\"]\n",
    "    for image_path in images_path:\n",
    "        image_name = image_path.split(\"/\")[-1]\n",
    "        key = 'results/' + image_name\n",
    "        upload_to_s3(image_path, bucket, key, region)\n",
    "        single_response = {\n",
    "            \"image_s3_path\" : {\n",
    "                \"bucket\" : bucket,\n",
    "                \"region\" : region,\n",
    "                \"key\" : key,\n",
    "\n",
    "            },\n",
    "        }\n",
    "        response[\"predictions\"].append(single_response)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def handler(data, context):\n",
    "   \"\"\"\n",
    "   data:\n",
    "   {\n",
    "        \"image_input\":\n",
    "        \"prompt\":\n",
    "        \"negative_prompt\":\n",
    "        \"weight\":\n",
    "        \"height\":\n",
    "        \"num_inference_steps\":\n",
    "        \"guidance_scale\":\n",
    "        \"cosine_scale_1\":\n",
    "        \"cosine_scale_2\":\n",
    "        \"cosine_scale_3\":\n",
    "        \"sigma\":\n",
    "        \"seed\":\n",
    "        \"bucket\":\n",
    "        \"region\":\n",
    "        \"key\":\n",
    "\n",
    "   } \n",
    "   \"\"\"\n",
    "   images_path, model_input = process_input(data)\n",
    "   response = process_output(model_input, images_path)\n",
    "\n",
    "   return json.dumps(response, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2432f1-9f18-4b3f-9f25-9d90f6d5079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 30 20:15:13 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   36C    P8              11W /  72W |      0MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7090e6e3-bdc2-421e-b0dd-cba8983d1230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body: {'prompt': 'a satellite image', 'negative_prompt': 'blurry, ugly, duplicate, poorly drawn, deformed, mosaic', 'width': 2048, 'height': 2048, 'num_inference_steps': 50, 'guidance_scale': 7.5, 'cosine_scale_1': 3, 'cosine_scale_2': 1, 'cosine_scale_3': 1, 'sigma': 0.8, 'view_batch_size': 16, 'stride': 64, 'seed': 2013, 'bucket': 'test-aws-mybucket', 'region': 'us-west-2', 'key': 'data/sample.png'}\n",
      "./tmp/sample.png exists already\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11062d8cf92440e7a96dbe2ad98fbf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Encoding Real Image ###\n",
      "### Phase 1 Denoising ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd09f35f665b490d8b634d64ec66f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Phase 1 Decoding ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6a57728900436d9cf6346f7e91b8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Phase 2 Denoising ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73d26305c62427bbcc9498280ec4cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Phase 2 Decoding ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f7a7f393da4b47aca99784473c24a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to s3://test-aws-mybucket/results/image_0.png...\n",
      "S3 upload successful! \n",
      "\n",
      "Uploading to s3://test-aws-mybucket/results/image_1.png...\n",
      "S3 upload successful! \n",
      "\n",
      "Uploading to s3://test-aws-mybucket/results/image_2.png...\n",
      "S3 upload successful! \n",
      "\n",
      "{\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"image_s3_path\": {\n",
      "        \"bucket\": \"test-aws-mybucket\",\n",
      "        \"region\": \"us-west-2\",\n",
      "        \"key\": \"results/image_0.png\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"image_s3_path\": {\n",
      "        \"bucket\": \"test-aws-mybucket\",\n",
      "        \"region\": \"us-west-2\",\n",
      "        \"key\": \"results/image_1.png\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"image_s3_path\": {\n",
      "        \"bucket\": \"test-aws-mybucket\",\n",
      "        \"region\": \"us-west-2\",\n",
      "        \"key\": \"results/image_2.png\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "payload = { \n",
    "    \"prompt\": \"a satellite image\",\n",
    "    \"negative_prompt\": \"blurry, ugly, duplicate, poorly drawn, deformed, mosaic\",\n",
    "    \"width\": 2048,\n",
    "    \"height\": 2048,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"cosine_scale_1\": 3,\n",
    "    \"cosine_scale_2\": 1,\n",
    "    \"cosine_scale_3\": 1,\n",
    "    \"sigma\": 0.8,\n",
    "    \"view_batch_size\": 16,\n",
    "    \"stride\": 64,\n",
    "    \"seed\": 2013,\n",
    "    \"bucket\": \"test-aws-mybucket\",\n",
    "    \"region\": \"us-west-2\",\n",
    "    \"key\": \"data/sample.png\",\n",
    "}\n",
    "\n",
    "print(handler(json.dumps(payload), None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da17140-7cc4-4e4c-9fad-384af0097e2a",
   "metadata": {},
   "source": [
    "## Inference Endpoint Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fca77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "# Specify your AWS Region\n",
    "aws_region='us-west-2'\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Role to give SageMaker permission to access AWS services.\n",
    "sagemaker_role= \"arn:aws:iam::573944535954:role/Super_Resolution\"\n",
    "\n",
    "ecr_image = \"573944535954.dkr.ecr.us-west-2.amazonaws.com/super-resolution:latest\"\n",
    "instance_type = \"ml.g5.xlarge\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1acc926",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-west-2:573944535954:model/superresolution-demofusion\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13832\\1465081235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m create_model_response = sagemaker_client.create_model(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mModelName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     PrimaryContainer={\n\u001b[0;32m      4\u001b[0m         \u001b[1;34m'Image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mecr_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         'Environment': {\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m                 )\n\u001b[0;32m    564\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1015\u001b[0m             )\n\u001b[0;32m   1016\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-west-2:573944535954:model/superresolution-demofusion\"."
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = 'demofusion-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': ecr_image,\n",
    "    },\n",
    "    ExecutionRoleArn = sagemaker_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b77bbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the endpoint configuration associated with this endpoint.\n",
    "endpoint_config_name = \"superresolution-demofusion-config\"\n",
    "\n",
    "create_endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"AllTraffic\", # The name of the production variant.\n",
    "            \"ModelName\": model_name, \n",
    "            \"InstanceType\": instance_type, # Specify the compute instance type.\n",
    "            \"InitialInstanceCount\": 1 # Number of instances to launch initially.\n",
    "        }\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            # Location to upload response outputs when no location is provided in the request.\n",
    "            \"S3OutputPath\": \"s3://test-aws-mybucket/results/\"\n",
    "            },        \n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a64b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the endpoint.The name must be unique within an AWS Region in your AWS account.\n",
    "endpoint_name = 'super-resolution-demofusion' \n",
    "\n",
    "create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "                                            EndpointName=endpoint_name, \n",
    "                                            EndpointConfigName=endpoint_config_name) \n",
    "\n",
    "resp = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Endpoint Status: \" + status)\n",
    "\n",
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sagemaker_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab625966",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpointAsync operation: Endpoint super-resolution-demofusion of account 573944535954 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13832\\2003616249.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# services, your client applications use this API to get inferences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# from the model hosted at the specified endpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m response = sagemaker_runtime.invoke_endpoint_async(\n\u001b[0m\u001b[0;32m     12\u001b[0m                             \u001b[0mEndpointName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                             \u001b[0mContentType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'application/json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m                 )\n\u001b[0;32m    564\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1015\u001b[0m             )\n\u001b[0;32m   1016\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpointAsync operation: Endpoint super-resolution-demofusion of account 573944535954 not found."
     ]
    }
   ],
   "source": [
    "# Create a low-level client representing Amazon SageMaker Runtime\n",
    "sagemaker_runtime = boto3.client(\"sagemaker-runtime\", region_name=aws_region)\n",
    "\n",
    "# Specify the location of the input. Here, a single SVM sample\n",
    "input_location = \"s3://test-aws-mybucket/payload.json\"\n",
    "\n",
    "\n",
    "# After you deploy a model into production using SageMaker hosting \n",
    "# services, your client applications use this API to get inferences \n",
    "# from the model hosted at the specified endpoint.\n",
    "response = sagemaker_runtime.invoke_endpoint_async(\n",
    "                            EndpointName=endpoint_name, \n",
    "                            ContentType='application/json',\n",
    "                            InputLocation=input_location,\n",
    "                            InvocationTimeoutSeconds=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sagemaker_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e03aa",
   "metadata": {},
   "source": [
    "## Inference Endpoint Deployment Version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a47d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# Specify your AWS Region\n",
    "aws_region='us-west-2'\n",
    "\n",
    "# Role to give SageMaker permission to access AWS services.\n",
    "sagemaker_role= \"arn:aws:iam::573944535954:role/Super_Resolution\"\n",
    "\n",
    "ecr_image = \"573944535954.dkr.ecr.us-west-2.amazonaws.com/super-resolution:latest\"\n",
    "model_name = \"superresolution-demofusion\"\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "endpoint_name = 'super-resolution-demofusion' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2731fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of your endpoint\n",
    "endpoint_name='super-resolution-demofusion'\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Delete endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "delete_config_response = sagemaker_client.delete_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "981716ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: superresolution-demofusion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint super-resolution-demofusion: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13832\\4167828342.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m predictor = estimator.deploy(\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sagemaker\\model.py\u001b[0m in \u001b[0;36mdeploy\u001b[1;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, **kwargs)\u001b[0m\n\u001b[0;32m   1747\u001b[0m                 \u001b[0mexplainer_config_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_request_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             self.sagemaker_session.endpoint_from_production_variants(\n\u001b[0m\u001b[0;32m   1750\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[0mproduction_variants\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduction_variant\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[1;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[0;32m   5726\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5728\u001b[1;33m         return self.create_endpoint(\n\u001b[0m\u001b[0;32m   5729\u001b[0m             \u001b[0mendpoint_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5730\u001b[0m             \u001b[0mconfig_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[1;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[0;32m   4584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4585\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4586\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlive_logging\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlive_logging\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sagemaker\\session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[1;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[0;32m   5369\u001b[0m                     \u001b[0mactual_status\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5370\u001b[0m                 )\n\u001b[1;32m-> 5371\u001b[1;33m             raise exceptions.UnexpectedStatusException(\n\u001b[0m\u001b[0;32m   5372\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5373\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"InService\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint super-resolution-demofusion: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=\"s3://test-aws-mybucket/results/\",\n",
    "    failure_path=\"s3://test-aws-mybucket/results/\"\n",
    ")\n",
    "\n",
    "estimator = Model(\n",
    "    name=model_name,\n",
    "    image_uri=ecr_image,\n",
    "    role=sagemaker_role,\n",
    "    source_dir=\"/opt/ml/code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    1, \n",
    "    instance_type, \n",
    "    endpoint_name=endpoint_name, \n",
    "    async_inference_config=async_config,\n",
    "    container_startup_health_check_timeout= 600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sess.sagemaker_runtime_client\n",
    "\n",
    "payload = { \n",
    "    \"prompt\": \"a satellite image\",\n",
    "    \"negative_prompt\": \"blurry, ugly, duplicate, poorly drawn, deformed, mosaic\",\n",
    "    \"width\": 2048,\n",
    "    \"height\": 2048,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"cosine_scale_1\": 3,\n",
    "    \"cosine_scale_2\": 1,\n",
    "    \"cosine_scale_3\": 1,\n",
    "    \"sigma\": 0.8,\n",
    "    \"view_batch_size\": 16,\n",
    "    \"stride\": 64,\n",
    "    \"seed\": 2013,\n",
    "    \"bucket\": \"test-aws-mybucket\",\n",
    "    \"region\": \"us-west-2\",\n",
    "    \"key\": \"data/sample.png\",\n",
    "}\n",
    "response = sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "r = response[\"Body\"]\n",
    "print(\"RESULT r.read().decode():\", r.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6033f03",
   "metadata": {},
   "source": [
    "## Inference local endpoint deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pprint boto3 sagemaker json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a155a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import boto3\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "image_uri_inference = \"573944535954.dkr.ecr.us-west-2.amazonaws.com/super-resolution:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "instance_type_local = \"local_gpu\"\n",
    "\n",
    "session_local = LocalSession()\n",
    "session_local.config = {instance_type_local: {\"local_code\": True}}\n",
    "print(type(session_local))\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Model\n",
    "\n",
    "\n",
    "estimator = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    role=role,\n",
    "    source_dir=\"/opt/ml/code\",\n",
    "    entry_point=\"inference.py\", # this argument is used to override internal container entrypoint, if needed!\n",
    "    sagemaker_session=session_local,  # local session\n",
    "    #                   predictor_cls=None,\n",
    "    #                   env=None,\n",
    "    #                   name=None,\n",
    "    #                   vpc_config=None,\n",
    "    #                   enable_network_isolation=False,\n",
    "    #                   model_kms_key=None,\n",
    "    #                   image_config=None,\n",
    "    #                   code_location=None,\n",
    "    #                   container_log_level=20,\n",
    "    #                   dependencies=None,\n",
    "    #                   git_config=None\n",
    ")\n",
    "\n",
    "predictor = estimator.deploy(1, instance_type_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {instance_type_local: {\"local_code\": True}}\n",
    "\n",
    "sm_client = sagemaker_session.sagemaker_runtime_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = { \n",
    "    \"prompt\": \"a satellite image\",\n",
    "    \"negative_prompt\": \"blurry, ugly, duplicate, poorly drawn, deformed, mosaic\",\n",
    "    \"width\": 2048,\n",
    "    \"height\": 2048,\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"cosine_scale_1\": 3,\n",
    "    \"cosine_scale_2\": 1,\n",
    "    \"cosine_scale_3\": 1,\n",
    "    \"sigma\": 0.8,\n",
    "    \"view_batch_size\": 16,\n",
    "    \"stride\": 64,\n",
    "    \"seed\": 2013,\n",
    "    \"bucket\": \"test-aws-mybucket\",\n",
    "    \"region\": \"us-west-2\",\n",
    "    \"key\": \"data/sample.png\",\n",
    "}\n",
    "\n",
    "\n",
    "response = sm_client.invoke_endpoint(\n",
    "    EndpointName=\"local-endpoint\",\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload),\n",
    ")\n",
    "\n",
    "r = response[\"Body\"]\n",
    "print(\"RESULT r.read().decode():\", r.read().decode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
